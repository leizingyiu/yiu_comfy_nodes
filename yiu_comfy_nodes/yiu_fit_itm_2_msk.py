import torch
import numpy as np
from PIL import Image, ImageOps
from .utils import get_current_language, build_types,get_localized_tooltips
from .yiu_base_node import YiuBaseNode

print("this is yiu_fit_item_to_mask.py - 202507051718")

class yiuFitItm2Msk(YiuBaseNode):
    

    tooltips_dict = {
        "zh-CN":{"input": {
            "item_image": "äº§å“å›¾ç‰‡ï¼Œå¯å…ˆç»è¿‡æŠ å›¾èŠ‚ç‚¹å¤„ç†åŽæŽ¥å…¥",
            "item_mask": "äº§å“å›¾ç‰‡é®ç½©ï¼Œå¯ä»ŽæŠ å›¾èŠ‚ç‚¹èŽ·å–",
            "canvas_image": "èƒŒæ™¯å›¾ç‰‡ï¼Œå»ºè®®ä½¿ç”¨åœºæ™¯å‚è€ƒå›¾ç‰‡",
            "canvas_mask": "èƒŒæ™¯é®ç½©ï¼Œäº§å“éœ€è¦æ”¾åœ¨èƒŒæ™¯ä¸­çš„ä»€ä¹ˆä½ç½®ï¼Œå°±åœ¨èƒŒæ™¯é®ç½©ä¸­ç»˜åˆ¶",
            "object_fit": "äº§å“åœ¨é®ç½©å°ºå¯¸ä¸­ï¼Œå¡«å……æˆ–é€‚åº”",
            "bg_color": "è¾“å‡ºæ—¶ä½¿ç”¨çš„èƒŒæ™¯é¢œè‰²ï¼Œé»˜è®¤é»‘è‰²",
        },
        "output":{
            "item_on_transparent":"è°ƒæ•´å¥½ä½ç½®å’Œç¼©æ”¾çš„äº§å“ï¼Œåœ¨é€æ˜ŽèƒŒæ™¯ä¸Š",
              "item_mask":"è°ƒæ•´å¥½ä½ç½®å’Œç¼©æ”¾çš„äº§å“é®ç½©",
                "item_on_bgcolor":"è°ƒæ•´å¥½ä½ç½®å’Œç¼©æ”¾çš„äº§å“é®ç½©ï¼Œåœ¨èƒŒæ™¯é¢œè‰²ä¸Š",
                  "item_on_canvas":"è°ƒæ•´å¥½ä½ç½®å’Œç¼©æ”¾çš„äº§å“é®ç½©ï¼Œåœ¨åŽŸèƒŒæ™¯å›¾ç‰‡ä¸Š",
                  "help_text":"å¸®åŠ©ä¿¡æ¯"
        }},
        "en":{"input": {
            "item_image": "Product image, recommended to process with a cutout node first",
            "item_mask": "Product image mask, can be obtained from the cutout node",
            "canvas_image": "Background image, suggested to use a scene reference image",
            "canvas_mask": "Background mask, draw where the product should be placed in the background",
            "object_fit": "How the product fits inside the mask area: fill or fit",
            "bg_color": "Background color for output, defaults to black"
        },
        "output":{
            "item_on_transparent":"Adjust the position and scale of the product, on a transparent background",
            "item_mask":"Adjust the position and scale of the product mask",
            "item_on_bgcolor":"Adjust the position and scale of the product mask, on the background color",
            "item_on_canvas":"Adjust the position and scale of the product mask, on the original background image",
            "help_text":"Help Text"
        }
        }
    }

    base_inputs = {
            "item_image": ("IMAGE", {}),
            "item_mask": ("MASK", {}),
            "canvas_image": ("IMAGE", {}),
            "canvas_mask": ("MASK", {}),
            "object_fit": (["fill", "fit"], {"default": "fit"}),
            "bg_color": ("STRING", {
                "default": "#000000", 
                "pattern": "^#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})$"
            }),
        }

    base_outputs={
        "item_on_transparent":"IMAGE",
         "item_mask":"MASK",
         "item_on_bgcolor":"IMAGE",
         "item_on_canvas":"IMAGE",
        "help_text":"STRING"
    }


    MAIN='yiu_fit_item_to_mask'

    @classmethod
    def DISPLAY_NAME(cls):
        return "ðŸ–Šï¸ Yiu Fit Item To Mask"
    
    @classmethod
    def get_help_text(self):
        lang = get_current_language()
        help_texts = {
            "zh-CN": "ðŸ“– å°†å›¾1(item)ç‰©å“æ”¾ç½®åœ¨å›¾2(canvas)çš„é®ç½©ä¸­ã€‚ç‰©å“(item)å›¾ç‰‡å»ºè®®å…ˆæŠ å›¾ï¼Œé®ç½©(canvas_mask)å†³å®šç‰©å“ä½ç½®ã€‚",
            "en": "ðŸ“– Place the item in Figure 1 (item) in the mask in Figure 2 (canvas). It is recommended to cut out the item image first, and the mask (canvas_mask) determines the position of the item."
        }
        return  get_localized_tooltips(self.tooltips_dict,lang)+'\n'+help_texts.get(lang, help_texts["en"])
    
    @classmethod
    def yiu_fit_item_to_mask(self, item_image, item_mask, canvas_image, canvas_mask, object_fit, bg_color):
        b_size, height, width, canvas_channels = canvas_image.shape
        try:
            msk_width, msk_height, msk_left, msk_top = self.mask_bbox(canvas_image, canvas_mask)
        except ValueError as e:
            raise ValueError(f"è’™ç‰ˆè¾¹ç•Œæ¡†è®¡ç®—å¤±è´¥: {str(e)}") from e

        if not bg_color.startswith('#'):
            raise ValueError(f"èƒŒæ™¯é¢œè‰²å¿…é¡»ä»¥ '#' å¼€å¤´ï¼Œå½“å‰å€¼: {bg_color}")
        
        bg_color_hex = bg_color.lstrip('#')
        
        if len(bg_color_hex) == 3:
            r = int(bg_color_hex[0] * 2, 16)
            g = int(bg_color_hex[1] * 2, 16)
            b = int(bg_color_hex[2] * 2, 16)
        else:
            r = int(bg_color_hex[0:2], 16)
            g = int(bg_color_hex[2:4], 16)
            b = int(bg_color_hex[4:6], 16)

        batch_size, img_h, img_w, channels = item_image.shape
        image_np = (item_image[0].cpu().numpy() * 255).astype(np.uint8)
        new_width, new_height, x, y = self.fit_or_fill_bbox(msk_width, msk_height, msk_left, msk_top, img_w, img_h, object_fit)

        # è½¬æ¢ item ä¸º PIL å›¾åƒ
        image_np = np.squeeze(image_np)
        pil_item = Image.fromarray(image_np).convert("RGBA")

        resized_item = pil_item.resize((int(new_width), int(new_height)), Image.LANCZOS)

        # æž„å»ºé€æ˜Žåº•å›¾
        transparent_img = Image.new("RGBA", (width, height), (0, 0, 0, 0))
        transparent_img.paste(resized_item, (int(x), int(y)), mask=resized_item)

        # æ­£ç¡®å¤„ç† item çš„åŽŸå§‹ mask
        item_mask_np = item_mask[0].cpu().numpy()
        if item_mask_np.ndim == 3 and item_mask_np.shape[0] == 1:
            item_mask_np = np.squeeze(item_mask_np, axis=0)
        elif item_mask_np.ndim == 3 and item_mask_np.shape[-1] == 1:
            item_mask_np = np.squeeze(item_mask_np, axis=-1)

        # è½¬ä¸º PIL mask å¹¶ç¼©æ”¾
        pil_item_mask = Image.fromarray((item_mask_np * 255).astype(np.uint8))
        resized_mask = pil_item_mask.resize((int(new_width), int(new_height)), Image.LANCZOS)

        # ç²˜è´´åˆ° canvas å¯¹åº”ä½ç½®
        mask_img = Image.new("L", (width, height), 0)
        mask_img.paste(resized_mask, (int(x), int(y)))


        # èƒŒæ™¯é¢œè‰²å›¾åƒ
        bg_color_img = Image.new("RGBA", (width, height), (r, g, b, 255))
        composited_bgcolor = Image.alpha_composite(bg_color_img, transparent_img)

        # ç”¨ canvas_image ä½œä¸ºèƒŒæ™¯
        canvas_np = (canvas_image[0].cpu().numpy() * 255).astype(np.uint8)
        canvas_pil = Image.fromarray(np.squeeze(canvas_np)).convert("RGBA")
        composited_canvas = Image.alpha_composite(canvas_pil, transparent_img)

        # è½¬å›ž tensor
        def pil_to_tensor(pil_img):
            np_img = np.array(pil_img).astype(np.float32) / 255.0
            if np_img.ndim == 2:
                return torch.from_numpy(np_img).unsqueeze(0)  # (1, H, W)
            return torch.from_numpy(np_img).unsqueeze(0)  # (1, H, W, C)

        return (
            pil_to_tensor(transparent_img),                # item_on_transparent
            pil_to_tensor(mask_img),                       # item_mask
            pil_to_tensor(composited_bgcolor),             # item_on_bgcolor
            pil_to_tensor(composited_canvas),              # item_on_canvas
        )
    


    @classmethod
    def mask_bbox(self, image, mask):
        if isinstance(mask, torch.Tensor):
            mask = mask.cpu().numpy()
        mask = np.squeeze(mask)
        if isinstance(image, torch.Tensor):
            image = image.cpu().numpy()

        if image.shape[1:3] != mask.shape:
            raise ValueError(f"å°ºå¯¸ä¸åŒ¹é…: å›¾åƒå°ºå¯¸ {image.shape[1:3]} ä¸Žè’™ç‰ˆå°ºå¯¸ {mask.shape} ä¸ç¬¦")

        non_zero_indices = np.argwhere(mask > 0)
        if non_zero_indices.size == 0:
            raise ValueError("è’™ç‰ˆå†…æ— æœ‰æ•ˆåŒºåŸŸï¼Œå¯èƒ½æœªç»˜åˆ¶è’™ç‰ˆ")

        y_start, x_start = non_zero_indices.min(axis=0).tolist()
        y_end, x_end = non_zero_indices.max(axis=0).tolist()

        width = int(x_end - x_start + 1)
        height = int(y_end - y_start + 1)
        return width, height, int(x_start), int(y_start)

    @staticmethod
    def fit_or_fill_bbox(bw, bh, bx, by, rw, rh, object_fit):
        rect_ratio = rw / rh
        bbox_ratio = bw / bh

        if object_fit == 'fit':
            if rect_ratio > bbox_ratio:
                new_width = bw
                new_height = bw / rect_ratio
            else:
                new_height = bh
                new_width = bh * rect_ratio
        elif object_fit == 'fill':
            if rect_ratio > bbox_ratio:
                new_height = bh
                new_width = bh * rect_ratio
            else:
                new_width = bw
                new_height = bw / rect_ratio
        else:
            raise ValueError("object_fit must be 'fit' or 'fill'")

        new_left = bx + (bw - new_width) / 2
        new_top = by + (bh - new_height) / 2

        return new_width, new_height, new_left, new_top
